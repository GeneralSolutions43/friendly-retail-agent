# Specification: Streaming AI Responses

## Overview
Update the Friendly Retail Agent to support real-time token streaming. This will significantly reduce the perceived latency for users by displaying the agent's response as it is generated, rather than waiting for the entire completion.

## Functional Requirements
- **Streaming Backend:** Implement a new FastAPI endpoint (or update the existing `/chat`) to return a `StreamingResponse` using Server-Sent Events (SSE).
- **LLM Integration:** Configure LangChain/ChatGroq to stream tokens as they are generated by Llama 3.
- **Tone Support:** Ensure the streaming response respects the user-selected persona (Helpful Professional, Friendly Assistant, Expert Consultant).
- **Frontend Streaming Client:** Update the Next.js frontend to handle SSE and append incoming text chunks to the UI in real-time.

## User Experience (UX) Requirements
- **Typing Indicator:** Display a "Typing..." state or animation while waiting for the first chunk of data.
- **Smooth Content Rendering:** Incrementally render the text chunks into the agent's chat bubble.
- **Auto-Scroll:** Automatically scroll the chat container to the bottom as new content is added to ensure the latest tokens are visible.
- **Responsive Design:** Ensure the streaming interaction feels fluid on both desktop and mobile devices.

## Non-Functional Requirements
- **Latency:** Aim for the first token to be delivered within <500ms of the request.
- **Stability:** The stream should handle network interruptions gracefully.
- **Efficiency:** Minimize memory overhead on the backend for concurrent streams.

## Acceptance Criteria
- [ ] Backend provides a valid SSE stream of tokens for a given prompt and tone.
- [ ] Frontend successfully consumes the SSE stream and updates the chat UI incrementally.
- [ ] The "Typing..." indicator appears correctly and disappears when the first token arrives.
- [ ] The chat window auto-scrolls to follow the text stream.
- [ ] All three agent personas are correctly applied to the streaming output.

## Out of Scope
- Streaming for tool outputs (search results will still be gathered before the final response stream starts, for now).
- Multi-user broadcast streams (responses are strictly 1:1).
